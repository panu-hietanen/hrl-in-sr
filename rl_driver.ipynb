{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "from libs.srenv import SREnv\n",
    "from agents.rlagent import DQNAgent, ReplayBuffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1., 1., 1.],\n",
      "        [2., 2., 2., 2., 2.]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones(5)\n",
    "b = torch.ones(5) * 2\n",
    "\n",
    "print(torch.stack([a, b]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "library = {\n",
    "    '+': 2,\n",
    "    '-': 2,\n",
    "    '*': 2,\n",
    "    '/': 2,\n",
    "    # 'sin': 1,\n",
    "    # 'cos': 1,\n",
    "    'C': 0,  # Placeholder for constants\n",
    "}\n",
    "\n",
    "# Create your data and target tensors\n",
    "n_samples = 1000\n",
    "n_vars = 1\n",
    "\n",
    "for i in range(n_vars):\n",
    "    var_name = f'X{i}'\n",
    "    library[var_name] = 0\n",
    "\n",
    "diff = [torch.ones(n_samples) + i for i in range(n_vars)]\n",
    "data = torch.randn([n_vars, n_samples]) + torch.stack(diff) # Shape: (n_vars, n_samples)\n",
    "target = 2 * data[0] - 1\n",
    "\n",
    "# Initialize the environment\n",
    "max_depth = 10\n",
    "env = SREnv(library=library, data=data, target=target, max_depth=max_depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000])\n",
      "torch.Size([1, 1000])\n"
     ]
    }
   ],
   "source": [
    "print(target.shape)\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define vocabulary\n",
    "vocab = list(library.keys()) + ['PAD']\n",
    "symbol_to_index = {symbol: idx for idx, symbol in enumerate(vocab)}\n",
    "index_to_symbol = {idx: symbol for symbol, idx in symbol_to_index.items()}\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# Maximum sequence length\n",
    "max_seq_length = max_depth\n",
    "\n",
    "def encode_state(state):\n",
    "    # Convert symbols to indices\n",
    "    state_indices = [symbol_to_index[symbol] for symbol in state]\n",
    "    # Pad sequence\n",
    "    if len(state_indices) < max_seq_length:\n",
    "        state_indices += [symbol_to_index['PAD']] * (max_seq_length - len(state_indices))\n",
    "    else:\n",
    "        state_indices = state_indices[:max_seq_length]\n",
    "    return torch.tensor(state_indices, dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_symbols = list(library.keys())\n",
    "action_size = len(action_symbols)\n",
    "symbol_to_action_idx = {symbol: idx for idx, symbol in enumerate(action_symbols)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['+', '-', '*', '/', 'C', 'X0']\n"
     ]
    }
   ],
   "source": [
    "print(action_symbols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 128\n",
    "hidden_dim = 256\n",
    "num_batches = 500\n",
    "num_episodes_per_batch = 10\n",
    "batch_quantile = 0.15\n",
    "batch_size = 250\n",
    "gamma = 0.99\n",
    "epsilon_start = 1.0\n",
    "epsilon_end = 0.1\n",
    "epsilon_decay = 0.95\n",
    "target_update = num_batches / 10\n",
    "memory_capacity = max_seq_length * num_episodes_per_batch * num_batches\n",
    "batch_eval = 5\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "embedding(): argument 'indices' (position 2) must be Tensor, not list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 81\u001b[0m\n\u001b[1;32m     78\u001b[0m dones_batch \u001b[38;5;241m=\u001b[39m       [item[\u001b[38;5;241m4\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m transitions \u001b[38;5;129;01min\u001b[39;00m top_episodes \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m transitions]\n\u001b[1;32m     80\u001b[0m \u001b[38;5;66;03m# Compute current Q-values\u001b[39;00m\n\u001b[0;32m---> 81\u001b[0m q_values \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstates_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     82\u001b[0m q_values \u001b[38;5;241m=\u001b[39m q_values\u001b[38;5;241m.\u001b[39mgather(\u001b[38;5;241m1\u001b[39m, actions_batch\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     83\u001b[0m \u001b[38;5;66;03m# Compute target Q-values\u001b[39;00m\n",
      "File \u001b[0;32m~/4yp/hrl-in-sr/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/4yp/hrl-in-sr/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/4yp/hrl-in-sr/agents/rlagent.py:50\u001b[0m, in \u001b[0;36mDQNAgent.forward\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, state):\n\u001b[0;32m---> 50\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m     lstm_out, (h_n, c_n) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlstm(x)\n\u001b[1;32m     52\u001b[0m     h_n \u001b[38;5;241m=\u001b[39m h_n[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[0;32m~/4yp/hrl-in-sr/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/4yp/hrl-in-sr/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/4yp/hrl-in-sr/.venv/lib/python3.10/site-packages/torch/nn/modules/sparse.py:190\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    192\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    193\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    195\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    196\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/4yp/hrl-in-sr/.venv/lib/python3.10/site-packages/torch/nn/functional.py:2551\u001b[0m, in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2545\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[1;32m   2546\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[1;32m   2547\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[1;32m   2548\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[1;32m   2549\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[1;32m   2550\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[0;32m-> 2551\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: embedding(): argument 'indices' (position 2) must be Tensor, not list"
     ]
    }
   ],
   "source": [
    "agent = DQNAgent(vocab_size, embedding_dim, hidden_dim, action_size)\n",
    "target_agent = DQNAgent(vocab_size, embedding_dim, hidden_dim, action_size)\n",
    "target_agent.load_state_dict(agent.state_dict())\n",
    "target_agent.eval()\n",
    "agent.train()\n",
    "\n",
    "optimizer = optim.Adam(agent.parameters(), lr=1e-4)\n",
    "criterion = nn.MSELoss()\n",
    "memory = ReplayBuffer(memory_capacity)\n",
    "\n",
    "epsilon = epsilon_start\n",
    "\n",
    "for batch in range(num_batches):\n",
    "    episodes = []\n",
    "    for episode in range(num_episodes_per_batch):\n",
    "        state_symbols = env.reset()\n",
    "        state_encoded = encode_state(state_symbols)  # Shape: (seq_length,)\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        transitions = []\n",
    "        i = 0\n",
    "\n",
    "        while not done and i < max_seq_length:\n",
    "            # Select action\n",
    "            action_idx = agent.act(state_encoded, epsilon)\n",
    "            action_symbol = action_symbols[action_idx]\n",
    "\n",
    "            try:\n",
    "                next_state_symbols, reward, done = env.step(action_symbol)\n",
    "            except ValueError as e:\n",
    "                reward = 0\n",
    "                done = True\n",
    "                next_state_symbols = state_symbols  # Remain in the same state\n",
    "\n",
    "            next_state_encoded = encode_state(next_state_symbols)\n",
    "            total_reward += reward\n",
    "\n",
    "            # Store transition in memory\n",
    "            transitions.append((\n",
    "                state_encoded,\n",
    "                action_idx,\n",
    "                reward,\n",
    "                next_state_encoded,\n",
    "                done\n",
    "            ))\n",
    "\n",
    "            state_encoded = next_state_encoded\n",
    "            state_symbols = next_state_symbols\n",
    "            \n",
    "            i += 1\n",
    "\n",
    "        if not done:\n",
    "            transitions = [\n",
    "                (\n",
    "                    t[0],\n",
    "                    t[1],\n",
    "                    -1,\n",
    "                    t[3],\n",
    "                    t[4]\n",
    "                )\n",
    "                for t in transitions\n",
    "            ]\n",
    "            total_reward = -1\n",
    "\n",
    "        episodes.append((transitions, total_reward))\n",
    "\n",
    "        total_rewards = [episode[1] for episode in episodes]\n",
    "\n",
    "        threshold = np.quantile(total_rewards, 1 - batch_quantile)\n",
    "\n",
    "        top_episodes = [episode[0] for episode in episodes if episode[1] >= threshold]\n",
    "\n",
    "        # Sample from memory\n",
    "        states_batch =      [item[0] for transitions in top_episodes for item in transitions]\n",
    "        actions_batch =     [item[1] for transitions in top_episodes for item in transitions]\n",
    "        rewards_batch =     [item[2] for transitions in top_episodes for item in transitions]\n",
    "        next_states_batch = [item[3] for transitions in top_episodes for item in transitions]\n",
    "        dones_batch =       [item[4] for transitions in top_episodes for item in transitions]\n",
    "\n",
    "        # Compute current Q-values\n",
    "        q_values = agent(states_batch)\n",
    "        q_values = q_values.gather(1, actions_batch.unsqueeze(1)).squeeze(1)\n",
    "        # Compute target Q-values\n",
    "        with torch.no_grad():\n",
    "            next_q_values = target_agent(next_states_batch).max(dim=1)[0]\n",
    "            target_q_values = rewards_batch + gamma * next_q_values * (1 - dones_batch)\n",
    "        # Compute loss\n",
    "        loss = criterion(q_values, target_q_values)\n",
    "        # Optimize the model\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if done:\n",
    "            print(f\"Episode {batch}.{episode} completed, Total Reward: {total_reward}\")\n",
    "        else:\n",
    "            print(f\"Episode {batch}.{episode} failed, Total Reward: {total_reward}\")\n",
    "\n",
    "    # Decay epsilon\n",
    "    if epsilon > epsilon_end:\n",
    "        epsilon *= epsilon_decay\n",
    "\n",
    "    # Update target network\n",
    "    if episode % target_update == 0:\n",
    "        target_agent.load_state_dict(agent.state_dict())\n",
    "\n",
    "    if batch % batch_eval == 0:\n",
    "        # Testing the agent\n",
    "        print('---------------------')\n",
    "        print('Evaluating...')\n",
    "        print('---------------------')\n",
    "        state_symbols = env.reset()\n",
    "        state_encoded = encode_state(state_symbols)\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        expression_actions = []\n",
    "        i = 0\n",
    "        agent.eval()\n",
    "        \n",
    "        while not done and i < max_seq_length:\n",
    "            with torch.no_grad():\n",
    "                q_values = agent(state_encoded.unsqueeze(0))  # Add batch dimension\n",
    "                action_idx = torch.argmax(q_values).item()\n",
    "            action_symbol = action_symbols[action_idx]\n",
    "            expression_actions.append(action_symbol)\n",
    "            try:\n",
    "                next_state_symbols, reward, done = env.step(action_symbol)\n",
    "            except ValueError as e:\n",
    "                reward = -1.0\n",
    "                done = True\n",
    "                next_state_symbols = state_symbols\n",
    "            next_state_encoded = encode_state(next_state_symbols)\n",
    "            total_reward += reward\n",
    "            state_encoded = next_state_encoded\n",
    "            state_symbols = next_state_symbols\n",
    "\n",
    "            i += 1\n",
    "\n",
    "        if done and total_reward == 1:\n",
    "            print('Found expression! Stopping early...')\n",
    "            break\n",
    "        else:\n",
    "            agent.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constructed Expression: X1\n",
      "Test Total Reward: 0.2616344094276428\n"
     ]
    }
   ],
   "source": [
    "# Testing the agent\n",
    "state_symbols = env.reset()\n",
    "state_encoded = encode_state(state_symbols)\n",
    "done = False\n",
    "total_reward = 0\n",
    "expression_actions = []\n",
    "max_restart = 100\n",
    "r = 0\n",
    "i = 0\n",
    "\n",
    "agent.eval()\n",
    "target_agent.eval()\n",
    "\n",
    "while not done and r < max_restart:\n",
    "    with torch.no_grad():\n",
    "        q_values = agent(state_encoded.unsqueeze(0))  # Add batch dimension\n",
    "        action_idx = torch.argmax(q_values).item()\n",
    "    action_symbol = action_symbols[action_idx]\n",
    "    expression_actions.append(action_symbol)\n",
    "    \n",
    "    try:\n",
    "        next_state_symbols, reward, done = env.step(action_symbol)\n",
    "    except ValueError as e:\n",
    "        reward = -1.0\n",
    "        done = True\n",
    "        next_state_symbols = state_symbols\n",
    "\n",
    "    next_state_encoded = encode_state(next_state_symbols)\n",
    "    total_reward += reward\n",
    "    state_encoded = next_state_encoded\n",
    "    state_symbols = next_state_symbols\n",
    "\n",
    "    if i == max_seq_length:\n",
    "        state_symbols = env.reset()\n",
    "        state_encoded = encode_state(state_symbols)\n",
    "        total_reward = 0\n",
    "        expression_actions = []\n",
    "        i = 0\n",
    "        r += 1\n",
    "        print('restarting...')\n",
    "    else:\n",
    "        i += 1\n",
    "\n",
    "\n",
    "n_const = env.expression.n_constants\n",
    "const_count = 0\n",
    "for idx, token in enumerate(expression_actions):\n",
    "    if const_count == n_const:\n",
    "        break\n",
    "    if token == 'C':\n",
    "        const_val = env.expression.optimized_constants[const_count].item()\n",
    "        expression_actions[idx] = str(round(const_val, 3))\n",
    "        const_count += 1\n",
    "\n",
    "print(f\"Constructed Expression: {' '.join(expression_actions)}\")\n",
    "print(f\"Test Total Reward: {total_reward}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
