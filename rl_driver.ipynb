{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "from libs.srenv import SREnv\n",
    "from agents.rlagent import DQNAgent, ReplayBuffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1., 1., 1.],\n",
      "        [2., 2., 2., 2., 2.]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones(5)\n",
    "b = torch.ones(5) * 2\n",
    "\n",
    "print(torch.stack([a, b]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "library = {\n",
    "    '+': 2,\n",
    "    '-': 2,\n",
    "    '*': 2,\n",
    "    '/': 2,\n",
    "    # 'sin': 1,\n",
    "    # 'cos': 1,\n",
    "    'C': 0,  # Placeholder for constants\n",
    "}\n",
    "\n",
    "# Create your data and target tensors\n",
    "n_samples = 1000\n",
    "n_vars = 2\n",
    "\n",
    "for i in range(n_vars):\n",
    "    var_name = f'X{i}'\n",
    "    library[var_name] = 0\n",
    "\n",
    "diff = [torch.ones(n_samples) + i for i in range(n_vars)]\n",
    "data = torch.randn([n_vars, n_samples]) + torch.stack(diff) # Shape: (n_vars, n_samples)\n",
    "target = 2 * data[1] - data[0]\n",
    "\n",
    "# Initialize the environment\n",
    "max_depth = 10\n",
    "env = SREnv(library=library, data=data, target=target, max_depth=max_depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000])\n",
      "torch.Size([2, 1000])\n"
     ]
    }
   ],
   "source": [
    "print(target.shape)\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define vocabulary\n",
    "vocab = list(library.keys()) + ['PAD']\n",
    "symbol_to_index = {symbol: idx for idx, symbol in enumerate(vocab)}\n",
    "index_to_symbol = {idx: symbol for symbol, idx in symbol_to_index.items()}\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# Maximum sequence length\n",
    "max_seq_length = max_depth\n",
    "\n",
    "def encode_state(state):\n",
    "    # Convert symbols to indices\n",
    "    state_indices = [symbol_to_index[symbol] for symbol in state]\n",
    "    # Pad sequence\n",
    "    if len(state_indices) < max_seq_length:\n",
    "        state_indices += [symbol_to_index['PAD']] * (max_seq_length - len(state_indices))\n",
    "    else:\n",
    "        state_indices = state_indices[:max_seq_length]\n",
    "    return torch.tensor(state_indices, dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_symbols = list(library.keys())\n",
    "action_size = len(action_symbols)\n",
    "symbol_to_action_idx = {symbol: idx for idx, symbol in enumerate(action_symbols)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['+', '-', '*', '/', 'C', 'X0', 'X1']\n"
     ]
    }
   ],
   "source": [
    "print(action_symbols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 128\n",
    "hidden_dim = 256\n",
    "num_batches = 500\n",
    "num_episodes_per_batch = 10\n",
    "batch_quantile = 0.15\n",
    "batch_size = 250\n",
    "gamma = 0.99\n",
    "epsilon_start = 1.0\n",
    "epsilon_end = 0.1\n",
    "epsilon_decay = 0.95\n",
    "target_update = num_batches / 10\n",
    "memory_capacity = max_seq_length * num_episodes_per_batch * num_batches\n",
    "batch_eval = 5\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = DQNAgent(vocab_size, embedding_dim, hidden_dim, action_size)\n",
    "target_agent = DQNAgent(vocab_size, embedding_dim, hidden_dim, action_size)\n",
    "target_agent.load_state_dict(agent.state_dict())\n",
    "target_agent.eval()\n",
    "agent.train()\n",
    "\n",
    "optimizer = optim.Adam(agent.parameters(), lr=1e-4)\n",
    "criterion = nn.MSELoss()\n",
    "memory = ReplayBuffer(memory_capacity)\n",
    "\n",
    "epsilon = epsilon_start\n",
    "\n",
    "for batch in range(num_batches):\n",
    "    episodes = []\n",
    "    for episode in range(num_episodes_per_batch):\n",
    "        state_symbols = env.reset()\n",
    "        state_encoded = encode_state(state_symbols)  # Shape: (seq_length,)\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        transitions = []\n",
    "        i = 0\n",
    "\n",
    "        while not done and i < max_seq_length:\n",
    "            # Select action\n",
    "            action_idx = agent.act(state_encoded, epsilon)\n",
    "            action_symbol = action_symbols[action_idx]\n",
    "\n",
    "            try:\n",
    "                next_state_symbols, reward, done = env.step(action_symbol)\n",
    "            except ValueError as e:\n",
    "                reward = 0\n",
    "                done = True\n",
    "                next_state_symbols = state_symbols  # Remain in the same state\n",
    "\n",
    "            next_state_encoded = encode_state(next_state_symbols)\n",
    "            total_reward += reward\n",
    "\n",
    "            # Store transition in memory\n",
    "            transitions.append((\n",
    "                state_encoded,\n",
    "                action_idx,\n",
    "                reward,\n",
    "                next_state_encoded,\n",
    "                done\n",
    "            ))\n",
    "\n",
    "            state_encoded = next_state_encoded\n",
    "            state_symbols = next_state_symbols\n",
    "            \n",
    "            i += 1\n",
    "\n",
    "        if not done:\n",
    "            transitions = [\n",
    "                (\n",
    "                    t[0],\n",
    "                    t[1],\n",
    "                    -1,\n",
    "                    t[3],\n",
    "                    t[4]\n",
    "                )\n",
    "                for t in transitions\n",
    "            ]\n",
    "            total_reward = -1\n",
    "\n",
    "        episodes.append((transitions, total_reward))\n",
    "\n",
    "        total_rewards = [episode[1] for episode in episodes]\n",
    "\n",
    "        threshold = np.quantile(total_rewards, 1 - batch_quantile)\n",
    "\n",
    "        top_episodes = [episode[0] for episode in episodes if episode[1] >= threshold]\n",
    "\n",
    "        # Sample from memory\n",
    "        states_batch = [episode[0][0][0] for episode in top_episodes]\n",
    "        actions_batch = [episode[0][0][1] for episode in top_episodes]\n",
    "        rewards_batch = [episode[0][0][2] for episode in top_episodes]\n",
    "        next_states_batch = [episode[0][0][3] for episode in top_episodes]\n",
    "        dones_batch = [episode[0][0][4] for episode in top_episodes]\n",
    "\n",
    "        # Compute current Q-values\n",
    "        q_values = agent(states_batch)\n",
    "        q_values = q_values.gather(1, actions_batch.unsqueeze(1)).squeeze(1)\n",
    "        # Compute target Q-values\n",
    "        with torch.no_grad():\n",
    "            next_q_values = target_agent(next_states_batch).max(dim=1)[0]\n",
    "            target_q_values = rewards_batch + gamma * next_q_values * (1 - dones_batch)\n",
    "        # Compute loss\n",
    "        loss = criterion(q_values, target_q_values)\n",
    "        # Optimize the model\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if done:\n",
    "            print(f\"Episode {batch}.{episode} completed, Total Reward: {total_reward}\")\n",
    "        else:\n",
    "            print(f\"Episode {batch}.{episode} failed, Total Reward: {total_reward}\")\n",
    "\n",
    "    # Decay epsilon\n",
    "    if epsilon > epsilon_end:\n",
    "        epsilon *= epsilon_decay\n",
    "\n",
    "    # Update target network\n",
    "    if episode % target_update == 0:\n",
    "        target_agent.load_state_dict(agent.state_dict())\n",
    "\n",
    "    if batch % batch_eval == 0:\n",
    "        # Testing the agent\n",
    "        print('---------------------')\n",
    "        print('Evaluating...')\n",
    "        print('---------------------')\n",
    "        state_symbols = env.reset()\n",
    "        state_encoded = encode_state(state_symbols)\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        expression_actions = []\n",
    "        i = 0\n",
    "        agent.eval()\n",
    "        \n",
    "        while not done and i < max_seq_length:\n",
    "            with torch.no_grad():\n",
    "                q_values = agent(state_encoded.unsqueeze(0))  # Add batch dimension\n",
    "                action_idx = torch.argmax(q_values).item()\n",
    "            action_symbol = action_symbols[action_idx]\n",
    "            expression_actions.append(action_symbol)\n",
    "            try:\n",
    "                next_state_symbols, reward, done = env.step(action_symbol)\n",
    "            except ValueError as e:\n",
    "                reward = -1.0\n",
    "                done = True\n",
    "                next_state_symbols = state_symbols\n",
    "            next_state_encoded = encode_state(next_state_symbols)\n",
    "            total_reward += reward\n",
    "            state_encoded = next_state_encoded\n",
    "            state_symbols = next_state_symbols\n",
    "\n",
    "            i += 1\n",
    "\n",
    "        if done and total_reward == 1:\n",
    "            print('Found expression! Stopping early...')\n",
    "            break\n",
    "        else:\n",
    "            agent.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constructed Expression: X1\n",
      "Test Total Reward: 0.2616344094276428\n"
     ]
    }
   ],
   "source": [
    "# Testing the agent\n",
    "state_symbols = env.reset()\n",
    "state_encoded = encode_state(state_symbols)\n",
    "done = False\n",
    "total_reward = 0\n",
    "expression_actions = []\n",
    "max_restart = 100\n",
    "r = 0\n",
    "i = 0\n",
    "\n",
    "agent.eval()\n",
    "target_agent.eval()\n",
    "\n",
    "while not done and r < max_restart:\n",
    "    with torch.no_grad():\n",
    "        q_values = agent(state_encoded.unsqueeze(0))  # Add batch dimension\n",
    "        action_idx = torch.argmax(q_values).item()\n",
    "    action_symbol = action_symbols[action_idx]\n",
    "    expression_actions.append(action_symbol)\n",
    "    \n",
    "    try:\n",
    "        next_state_symbols, reward, done = env.step(action_symbol)\n",
    "    except ValueError as e:\n",
    "        reward = -1.0\n",
    "        done = True\n",
    "        next_state_symbols = state_symbols\n",
    "\n",
    "    next_state_encoded = encode_state(next_state_symbols)\n",
    "    total_reward += reward\n",
    "    state_encoded = next_state_encoded\n",
    "    state_symbols = next_state_symbols\n",
    "\n",
    "    if i == max_seq_length:\n",
    "        state_symbols = env.reset()\n",
    "        state_encoded = encode_state(state_symbols)\n",
    "        total_reward = 0\n",
    "        expression_actions = []\n",
    "        i = 0\n",
    "        r += 1\n",
    "        print('restarting...')\n",
    "    else:\n",
    "        i += 1\n",
    "\n",
    "\n",
    "n_const = env.expression.n_constants\n",
    "const_count = 0\n",
    "for idx, token in enumerate(expression_actions):\n",
    "    if const_count == n_const:\n",
    "        break\n",
    "    if token == 'C':\n",
    "        const_val = env.expression.optimized_constants[const_count].item()\n",
    "        expression_actions[idx] = str(round(const_val, 3))\n",
    "        const_count += 1\n",
    "\n",
    "print(f\"Constructed Expression: {' '.join(expression_actions)}\")\n",
    "print(f\"Test Total Reward: {total_reward}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
