{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "\n",
    "from libs.srenv import SREnv\n",
    "from agents.rlagent import DQNAgent, ReplayBuffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "library = {\n",
    "    '+': 2,\n",
    "    '-': 2,\n",
    "    '*': 2,\n",
    "    '/': 2,\n",
    "    'sin': 1,\n",
    "    'cos': 1,\n",
    "    'C': 0  # Placeholder for constants\n",
    "}\n",
    "\n",
    "# Create your data and target tensors\n",
    "n_samples = 1000\n",
    "n_vars = 1\n",
    "\n",
    "for i in range(n_vars):\n",
    "    var_name = f'X{i}'\n",
    "    library[var_name] = 0\n",
    "\n",
    "# X0 = torch.randn(-10, 10, n_samples)\n",
    "# X1 = torch.linspace(-10, 10, n_samples)\n",
    "data = torch.randn([n_vars, n_samples])  # Shape: (n_vars, n_samples)\n",
    "target = 2 * data[0] - 1\n",
    "\n",
    "# Initialize the environment\n",
    "max_depth = 10\n",
    "env = SREnv(library=library, data=data, target=target, max_depth=max_depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000])\n"
     ]
    }
   ],
   "source": [
    "print(target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define vocabulary\n",
    "vocab = list(library.keys()) + ['PAD']\n",
    "symbol_to_index = {symbol: idx for idx, symbol in enumerate(vocab)}\n",
    "index_to_symbol = {idx: symbol for symbol, idx in symbol_to_index.items()}\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# Maximum sequence length\n",
    "max_seq_length = max_depth\n",
    "\n",
    "def encode_state(state):\n",
    "    # Convert symbols to indices\n",
    "    state_indices = [symbol_to_index[symbol] for symbol in state]\n",
    "    # Pad sequence\n",
    "    if len(state_indices) < max_seq_length:\n",
    "        state_indices += [symbol_to_index['PAD']] * (max_seq_length - len(state_indices))\n",
    "    else:\n",
    "        state_indices = state_indices[:max_seq_length]\n",
    "    return torch.tensor(state_indices, dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_symbols = list(library.keys())\n",
    "action_size = len(action_symbols)\n",
    "symbol_to_action_idx = {symbol: idx for idx, symbol in enumerate(action_symbols)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['+', '-', '*', '/', 'sin', 'cos', 'C', 'X0']\n"
     ]
    }
   ],
   "source": [
    "print(action_symbols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 128\n",
    "hidden_dim = 256\n",
    "num_episodes = 300\n",
    "batch_size = 32\n",
    "gamma = 0.9\n",
    "epsilon_start = 1.0\n",
    "epsilon_end = 0.01\n",
    "epsilon_decay = 0.8\n",
    "target_update = 10\n",
    "memory_capacity = 10000\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0, Total Reward: 0\n",
      "Episode 1, Total Reward: 0\n",
      "Episode 2, Total Reward: 0\n",
      "Episode 3, Total Reward: 0\n",
      "Episode 4, Total Reward: 0.3476879894733429\n",
      "Episode 5, Total Reward: 0.20245149731636047\n",
      "Episode 6, Total Reward: 0\n",
      "Episode 7, Total Reward: 0.20245149731636047\n",
      "Episode 8, Total Reward: 0.20245152711868286\n",
      "Episode 9, Total Reward: 0\n",
      "Episode 10, Total Reward: 0\n",
      "Episode 11, Total Reward: 0.20245152711868286\n",
      "Episode 12, Total Reward: 0.12321987748146057\n",
      "Episode 13, Total Reward: 0\n",
      "Episode 14, Total Reward: 0.20245149731636047\n",
      "Episode 15, Total Reward: 0.8307622671127319\n",
      "Episode 16, Total Reward: 0.20188042521476746\n",
      "Episode 17, Total Reward: 0.20245149731636047\n",
      "Episode 18, Total Reward: 0\n",
      "Episode 19, Total Reward: 0\n",
      "Episode 20, Total Reward: 0.13594381511211395\n",
      "Episode 21, Total Reward: 0\n",
      "Episode 22, Total Reward: 0.20245152711868286\n",
      "Episode 23, Total Reward: 0\n",
      "Episode 24, Total Reward: 0\n",
      "Episode 25, Total Reward: 0\n",
      "Episode 26, Total Reward: 0\n",
      "Episode 27, Total Reward: 0.3476879894733429\n",
      "Episode 28, Total Reward: 0\n",
      "Episode 29, Total Reward: 0\n",
      "Episode 30, Total Reward: 0.20245149731636047\n",
      "Episode 31, Total Reward: 0\n",
      "Episode 32, Total Reward: 0\n",
      "Episode 33, Total Reward: 0.13196119666099548\n",
      "Episode 34, Total Reward: 0.20245149731636047\n",
      "Episode 35, Total Reward: 0\n",
      "Episode 36, Total Reward: 0.20245149731636047\n",
      "Episode 37, Total Reward: 0\n",
      "Episode 38, Total Reward: 0.20236223936080933\n",
      "Episode 39, Total Reward: 0.20031942427158356\n",
      "Episode 40, Total Reward: 0\n",
      "Episode 41, Total Reward: 0.1282702088356018\n",
      "Episode 42, Total Reward: 0.3476879894733429\n",
      "Episode 43, Total Reward: 0.5038129687309265\n",
      "Episode 44, Total Reward: 0.20732232928276062\n",
      "Episode 45, Total Reward: 0.18602663278579712\n",
      "Episode 46, Total Reward: 0.1313152313232422\n",
      "Episode 47, Total Reward: 0.3476879894733429\n",
      "Episode 48, Total Reward: 0.16357845067977905\n",
      "Episode 49, Total Reward: 0.3476879894733429\n",
      "Episode 50, Total Reward: 0\n",
      "Episode 51, Total Reward: 0.13213349878787994\n",
      "Episode 52, Total Reward: 0.20245149731636047\n",
      "Episode 53, Total Reward: 0\n",
      "Episode 54, Total Reward: 0.20245149731636047\n",
      "Episode 55, Total Reward: 0.20245149731636047\n",
      "Episode 56, Total Reward: 0.3476879894733429\n",
      "Episode 57, Total Reward: 0\n",
      "Episode 58, Total Reward: 0.3476879894733429\n",
      "Episode 59, Total Reward: 0.206107497215271\n",
      "Episode 60, Total Reward: 0.3476879894733429\n",
      "Episode 61, Total Reward: 0.3476879894733429\n",
      "Episode 62, Total Reward: 0.3476879894733429\n",
      "Episode 63, Total Reward: 0.5038129687309265\n",
      "Episode 64, Total Reward: 0.5038129687309265\n",
      "Episode 65, Total Reward: 1.0\n",
      "Episode 66, Total Reward: 0.3476879894733429\n",
      "Episode 67, Total Reward: 0.1313152313232422\n",
      "Episode 68, Total Reward: 0\n",
      "Episode 69, Total Reward: 0\n",
      "Episode 70, Total Reward: 0.1745728701353073\n",
      "Episode 71, Total Reward: 0.3476879894733429\n",
      "Episode 72, Total Reward: 0.26486703753471375\n",
      "Episode 73, Total Reward: 0\n",
      "Episode 74, Total Reward: 0\n",
      "Episode 75, Total Reward: 0\n",
      "Episode 76, Total Reward: 0.3476879894733429\n",
      "Episode 77, Total Reward: 0\n",
      "Episode 78, Total Reward: 0\n",
      "Episode 79, Total Reward: 0.26486703753471375\n",
      "Episode 80, Total Reward: 0.20245152711868286\n",
      "Episode 81, Total Reward: 0.3476879894733429\n",
      "Episode 82, Total Reward: 0\n",
      "Episode 83, Total Reward: 0\n",
      "Episode 84, Total Reward: 0\n",
      "Episode 85, Total Reward: 0\n",
      "Episode 86, Total Reward: 0.3476879894733429\n",
      "Episode 87, Total Reward: 0\n",
      "Episode 88, Total Reward: 0\n",
      "Episode 89, Total Reward: 0.3476879894733429\n",
      "Episode 90, Total Reward: 0\n",
      "Episode 91, Total Reward: 0.2025410681962967\n",
      "Episode 92, Total Reward: 0\n",
      "Episode 93, Total Reward: 0.20245149731636047\n",
      "Episode 94, Total Reward: 0.5038129687309265\n",
      "Episode 95, Total Reward: 0.05967364087700844\n",
      "Episode 96, Total Reward: 0\n",
      "Episode 97, Total Reward: 0\n",
      "Episode 98, Total Reward: 0.3476879894733429\n",
      "Episode 99, Total Reward: 0\n",
      "Episode 100, Total Reward: 0.20245152711868286\n",
      "Episode 101, Total Reward: 0.3476879894733429\n",
      "Episode 102, Total Reward: 0.20245149731636047\n",
      "Episode 103, Total Reward: 0.2881619930267334\n",
      "Episode 104, Total Reward: 0.3476879894733429\n",
      "Episode 105, Total Reward: 0.20245149731636047\n",
      "Episode 106, Total Reward: 0.3476879894733429\n",
      "Episode 107, Total Reward: 0\n",
      "Episode 108, Total Reward: 0.3476879894733429\n",
      "Episode 109, Total Reward: 0.3476879894733429\n",
      "Episode 110, Total Reward: 0.5038129687309265\n",
      "Episode 111, Total Reward: 0\n",
      "Episode 112, Total Reward: 0\n",
      "Episode 113, Total Reward: 0\n",
      "Episode 114, Total Reward: 0.5038129687309265\n",
      "Episode 115, Total Reward: 0.20245152711868286\n",
      "Episode 116, Total Reward: 0\n",
      "Episode 117, Total Reward: 0.3476879894733429\n",
      "Episode 118, Total Reward: 0\n",
      "Episode 119, Total Reward: 0.3476879894733429\n",
      "Episode 120, Total Reward: 0\n",
      "Episode 121, Total Reward: 0\n",
      "Episode 122, Total Reward: 1.0\n",
      "Episode 123, Total Reward: 0.3476879894733429\n",
      "Episode 124, Total Reward: 0.3476879894733429\n",
      "Episode 125, Total Reward: 0\n",
      "Episode 126, Total Reward: 0.8307622671127319\n",
      "Episode 127, Total Reward: 0.3476879894733429\n",
      "Episode 128, Total Reward: 0.16574278473854065\n",
      "Episode 129, Total Reward: 0.1013810783624649\n",
      "Episode 130, Total Reward: 0.29066845774650574\n",
      "Episode 131, Total Reward: 0.20245149731636047\n",
      "Episode 132, Total Reward: 0\n",
      "Episode 133, Total Reward: 0.1745728701353073\n",
      "Episode 134, Total Reward: 0\n",
      "Episode 135, Total Reward: 0\n",
      "Episode 136, Total Reward: 0\n",
      "Episode 137, Total Reward: 0\n",
      "Episode 138, Total Reward: 0\n",
      "Episode 139, Total Reward: 0\n",
      "Episode 140, Total Reward: 0\n",
      "Episode 141, Total Reward: 0\n",
      "Episode 142, Total Reward: 0\n",
      "Episode 143, Total Reward: 0\n",
      "Episode 144, Total Reward: 0\n",
      "Episode 145, Total Reward: 0.1745728701353073\n",
      "Episode 146, Total Reward: 0\n",
      "Episode 147, Total Reward: 0\n",
      "Episode 148, Total Reward: 0\n",
      "Episode 149, Total Reward: 0\n",
      "Episode 150, Total Reward: 0\n",
      "Episode 151, Total Reward: 0\n",
      "Episode 152, Total Reward: 0\n",
      "Episode 153, Total Reward: 0\n",
      "Episode 154, Total Reward: 0.5038129687309265\n",
      "Episode 155, Total Reward: 0.06044356897473335\n",
      "Episode 156, Total Reward: 0\n",
      "Episode 157, Total Reward: 0\n",
      "Episode 158, Total Reward: 0.20245152711868286\n",
      "Episode 159, Total Reward: 0.3476879894733429\n",
      "Episode 160, Total Reward: 0\n",
      "Episode 161, Total Reward: 0\n",
      "Episode 162, Total Reward: 0\n",
      "Episode 163, Total Reward: 0\n",
      "Episode 164, Total Reward: 0\n",
      "Episode 165, Total Reward: 0.3476879894733429\n",
      "Episode 166, Total Reward: 1.0\n",
      "Episode 167, Total Reward: 0.20245149731636047\n",
      "Episode 168, Total Reward: 0.5038129687309265\n",
      "Episode 169, Total Reward: 0.5038129687309265\n",
      "Episode 170, Total Reward: 1.0\n",
      "Episode 171, Total Reward: 0.5038129687309265\n",
      "Episode 172, Total Reward: 0\n",
      "Episode 173, Total Reward: 0\n",
      "Episode 174, Total Reward: 0.5007926821708679\n",
      "Episode 175, Total Reward: 0.20245152711868286\n",
      "Episode 176, Total Reward: 0.5038129687309265\n",
      "Episode 177, Total Reward: 0.20245152711868286\n",
      "Episode 178, Total Reward: 1.0\n",
      "Episode 179, Total Reward: 0.20245149731636047\n",
      "Episode 180, Total Reward: 0\n",
      "Episode 181, Total Reward: 0.20245152711868286\n",
      "Episode 182, Total Reward: 0.5038129091262817\n",
      "Episode 183, Total Reward: 0\n",
      "Episode 184, Total Reward: 0.20245149731636047\n",
      "Episode 185, Total Reward: 0.3476879894733429\n",
      "Episode 186, Total Reward: 0.20245149731636047\n",
      "Episode 187, Total Reward: 0.5038129687309265\n",
      "Episode 188, Total Reward: 0.20245149731636047\n",
      "Episode 189, Total Reward: 0\n",
      "Episode 190, Total Reward: 0.5038129687309265\n",
      "Episode 191, Total Reward: 0.20245149731636047\n",
      "Episode 192, Total Reward: 0.20245149731636047\n",
      "Episode 193, Total Reward: 0.8307622671127319\n",
      "Episode 194, Total Reward: 0\n",
      "Episode 195, Total Reward: 0\n",
      "Episode 196, Total Reward: 1.0\n",
      "Episode 197, Total Reward: 1.0\n",
      "Episode 198, Total Reward: 1.0\n",
      "Episode 199, Total Reward: 0.3476879894733429\n",
      "Episode 200, Total Reward: 0.3476879894733429\n",
      "Episode 201, Total Reward: 0.20110337436199188\n",
      "Episode 202, Total Reward: 0\n",
      "Episode 203, Total Reward: 0\n",
      "Episode 204, Total Reward: 0\n",
      "Episode 205, Total Reward: 0.3476879894733429\n",
      "Episode 206, Total Reward: 1.0\n",
      "Episode 207, Total Reward: 1.0\n",
      "Episode 208, Total Reward: 0.3476879894733429\n",
      "Episode 209, Total Reward: 0.14326784014701843\n",
      "Episode 210, Total Reward: 0.19695208966732025\n",
      "Episode 211, Total Reward: 1.0\n",
      "Episode 212, Total Reward: 0.5038129687309265\n",
      "Episode 213, Total Reward: 0\n",
      "Episode 214, Total Reward: 0.5038129687309265\n",
      "Episode 215, Total Reward: 1.0\n",
      "Episode 216, Total Reward: 0\n",
      "Episode 217, Total Reward: 0.19523732364177704\n",
      "Episode 218, Total Reward: 0.20245152711868286\n",
      "Episode 219, Total Reward: 0.174568772315979\n",
      "Episode 220, Total Reward: 0.2024514377117157\n",
      "Episode 221, Total Reward: 0.5038129687309265\n",
      "Episode 222, Total Reward: 1.0\n",
      "Episode 223, Total Reward: 0.44176149368286133\n",
      "Episode 224, Total Reward: 1.0\n",
      "Episode 225, Total Reward: 0.5038129687309265\n",
      "Episode 226, Total Reward: 1.0\n",
      "Episode 227, Total Reward: 1.0\n",
      "Episode 228, Total Reward: 0.15412990748882294\n",
      "Episode 229, Total Reward: 0\n",
      "Episode 230, Total Reward: 1.0\n",
      "Episode 231, Total Reward: 0\n",
      "Episode 232, Total Reward: 0.18071532249450684\n",
      "Episode 233, Total Reward: 0.5038129687309265\n",
      "Episode 234, Total Reward: 0.13939812779426575\n",
      "Episode 235, Total Reward: 0.6030101776123047\n",
      "Episode 236, Total Reward: 0.5038129687309265\n",
      "Episode 237, Total Reward: 0\n",
      "Episode 238, Total Reward: 0\n",
      "Episode 239, Total Reward: 0.5038129687309265\n",
      "Episode 240, Total Reward: 0.5038129687309265\n",
      "Episode 241, Total Reward: 0.3225972652435303\n",
      "Episode 242, Total Reward: 1.0\n",
      "Episode 243, Total Reward: 0.46757835149765015\n",
      "Episode 244, Total Reward: 0\n",
      "Episode 245, Total Reward: 0.5007926821708679\n",
      "Episode 246, Total Reward: 1.0\n",
      "Episode 247, Total Reward: 0.1871069371700287\n",
      "Episode 248, Total Reward: 0\n",
      "Episode 249, Total Reward: 0\n",
      "Episode 250, Total Reward: 1.0\n",
      "Episode 251, Total Reward: 1.0\n",
      "Episode 252, Total Reward: 0.5038129687309265\n",
      "Episode 253, Total Reward: 0\n",
      "Episode 254, Total Reward: 0.3225972652435303\n",
      "Episode 255, Total Reward: 0.5038129687309265\n",
      "Episode 256, Total Reward: 1.0\n",
      "Episode 257, Total Reward: 0.5038129687309265\n",
      "Episode 258, Total Reward: 0.20245149731636047\n",
      "Episode 259, Total Reward: 1.0\n",
      "Episode 260, Total Reward: 1.0\n",
      "Episode 261, Total Reward: 0\n",
      "Episode 262, Total Reward: 1.0\n",
      "Episode 263, Total Reward: 0.2776065468788147\n",
      "Episode 264, Total Reward: 1.0\n",
      "Episode 265, Total Reward: 1.0\n",
      "Episode 266, Total Reward: 1.0\n",
      "Episode 267, Total Reward: 1.0\n",
      "Episode 268, Total Reward: 0.20245146751403809\n",
      "Episode 269, Total Reward: 1.0\n",
      "Episode 270, Total Reward: 0.2881619930267334\n",
      "Episode 271, Total Reward: 1.0\n",
      "Episode 272, Total Reward: 1.0\n",
      "Episode 273, Total Reward: 0.20245152711868286\n",
      "Episode 274, Total Reward: 1.0\n",
      "Episode 275, Total Reward: 1.0\n",
      "Episode 276, Total Reward: 0.3476879894733429\n",
      "Episode 277, Total Reward: 0.1013810783624649\n",
      "Episode 278, Total Reward: 1.0\n",
      "Episode 279, Total Reward: 1.0\n",
      "Episode 280, Total Reward: 0.5038129687309265\n",
      "Episode 281, Total Reward: 0.2025415450334549\n",
      "Episode 282, Total Reward: 1.0\n",
      "Episode 283, Total Reward: 0.34061262011528015\n",
      "Episode 284, Total Reward: 0\n",
      "Episode 285, Total Reward: 0.2881619930267334\n",
      "Episode 286, Total Reward: 1.0\n",
      "Episode 287, Total Reward: 1.0\n",
      "Episode 288, Total Reward: 0.34034234285354614\n",
      "Episode 289, Total Reward: 1.0\n",
      "Episode 290, Total Reward: 0.3225972652435303\n",
      "Episode 291, Total Reward: 0\n",
      "Episode 292, Total Reward: 1.0\n",
      "Episode 293, Total Reward: 0.5038129091262817\n",
      "Episode 294, Total Reward: 1.0\n",
      "Episode 295, Total Reward: 1.0\n",
      "Episode 296, Total Reward: 0.4676220417022705\n",
      "Episode 297, Total Reward: 0.5038129687309265\n",
      "Episode 298, Total Reward: 0.9899582266807556\n",
      "Episode 299, Total Reward: 1.0\n"
     ]
    }
   ],
   "source": [
    "agent = DQNAgent(vocab_size, embedding_dim, hidden_dim, action_size)\n",
    "target_agent = DQNAgent(vocab_size, embedding_dim, hidden_dim, action_size)\n",
    "target_agent.load_state_dict(agent.state_dict())\n",
    "target_agent.train()\n",
    "agent.train()\n",
    "\n",
    "optimizer = optim.Adam(agent.parameters(), lr=1e-4)\n",
    "criterion = nn.MSELoss()\n",
    "memory = ReplayBuffer(memory_capacity)\n",
    "\n",
    "epsilon = epsilon_start\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    state_symbols = env.reset()\n",
    "    state_encoded = encode_state(state_symbols)  # Shape: (seq_length,)\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    i = 0\n",
    "\n",
    "    while not done and i < max_seq_length:\n",
    "        # Select action\n",
    "        action_idx = agent.act(state_encoded, epsilon)\n",
    "        action_symbol = action_symbols[action_idx]\n",
    "        \n",
    "        try:\n",
    "            next_state_symbols, reward, done = env.step(action_symbol)\n",
    "        except ValueError as e:\n",
    "            reward = 0\n",
    "            done = True\n",
    "            next_state_symbols = state_symbols  # Remain in the same state\n",
    "\n",
    "        next_state_encoded = encode_state(next_state_symbols)\n",
    "        total_reward += reward\n",
    "\n",
    "        # Store transition in memory\n",
    "        memory.push(\n",
    "            state_encoded,\n",
    "            action_idx,\n",
    "            reward,\n",
    "            next_state_encoded,\n",
    "            done\n",
    "        )\n",
    "\n",
    "        state_encoded = next_state_encoded\n",
    "        state_symbols = next_state_symbols\n",
    "\n",
    "        # Experience replay\n",
    "        if len(memory) >= batch_size:\n",
    "            # Sample from memory\n",
    "            states_batch, actions_batch, rewards_batch, next_states_batch, dones_batch = memory.sample(batch_size)\n",
    "            \n",
    "            # Compute current Q-values\n",
    "            q_values = agent(states_batch)\n",
    "            q_values = q_values.gather(1, actions_batch.unsqueeze(1)).squeeze(1)\n",
    "            \n",
    "            # Compute target Q-values\n",
    "            with torch.no_grad():\n",
    "                next_q_values = target_agent(next_states_batch).max(dim=1)[0]\n",
    "                target_q_values = rewards_batch + gamma * next_q_values * (1 - dones_batch)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = criterion(q_values, target_q_values)\n",
    "            \n",
    "            # Optimize the model\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        i += 1\n",
    "    \n",
    "    # Decay epsilon\n",
    "    if epsilon > epsilon_end:\n",
    "        epsilon *= epsilon_decay\n",
    "\n",
    "    # Update target network\n",
    "    if episode % target_update == 0:\n",
    "        target_agent.load_state_dict(agent.state_dict())\n",
    "\n",
    "    print(f\"Episode {episode}, Total Reward: {total_reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constructed Expression: + + X0 X0 -1.0\n",
      "Test Total Reward: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Testing the agent\n",
    "state_symbols = env.reset()\n",
    "state_encoded = encode_state(state_symbols)\n",
    "done = False\n",
    "total_reward = 0\n",
    "expression_actions = []\n",
    "\n",
    "agent.eval()\n",
    "target_agent.eval()\n",
    "\n",
    "while not done:\n",
    "    with torch.no_grad():\n",
    "        q_values = agent(state_encoded.unsqueeze(0))  # Add batch dimension\n",
    "        action_idx = torch.argmax(q_values).item()\n",
    "    action_symbol = action_symbols[action_idx]\n",
    "    expression_actions.append(action_symbol)\n",
    "    \n",
    "    try:\n",
    "        next_state_symbols, reward, done = env.step(action_symbol)\n",
    "    except ValueError as e:\n",
    "        reward = -1.0\n",
    "        done = True\n",
    "        next_state_symbols = state_symbols\n",
    "\n",
    "    next_state_encoded = encode_state(next_state_symbols)\n",
    "    total_reward += reward\n",
    "    state_encoded = next_state_encoded\n",
    "    state_symbols = next_state_symbols\n",
    "\n",
    "n_const = env.expression.n_constants\n",
    "const_count = 0\n",
    "for idx, token in enumerate(expression_actions):\n",
    "    if const_count == n_const:\n",
    "        break\n",
    "    if token == 'C':\n",
    "        const_val = env.expression.optimized_constants[const_count].item()\n",
    "        expression_actions[idx] = str(const_val)\n",
    "        const_count += 1\n",
    "\n",
    "print(f\"Constructed Expression: {' '.join(expression_actions)}\")\n",
    "print(f\"Test Total Reward: {total_reward}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
