{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "from libs.srenv import SREnv\n",
    "from agents.rlagent import DQNAgent, ReplayBuffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library = {\n",
    "    '+': 2,\n",
    "    '-': 2,\n",
    "    '*': 2,\n",
    "    '/': 2,\n",
    "    'sin': 1,\n",
    "    'cos': 1,\n",
    "    'C': 0,  # Placeholder for constants\n",
    "}\n",
    "\n",
    "# Create data and target tensors\n",
    "n_samples = 1000\n",
    "n_vars = 1\n",
    "\n",
    "for i in range(n_vars):\n",
    "    var_name = f'X{i}'\n",
    "    library[var_name] = 0\n",
    "\n",
    "diff = [torch.zeros(n_samples) + i for i in range(n_vars)]\n",
    "data = torch.randn([n_vars, n_samples]) + torch.stack(diff) # Shape: (n_vars, n_samples)\n",
    "target = 2 * data[0] + 10\n",
    "\n",
    "# Initialize the environment\n",
    "max_depth = 10\n",
    "env = SREnv(library=library, data=data, target=target, max_depth=max_depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(target.shape)\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define vocabulary\n",
    "vocab = list(library.keys()) + ['PAD']\n",
    "symbol_to_index = {symbol: idx for idx, symbol in enumerate(vocab)}\n",
    "index_to_symbol = {idx: symbol for symbol, idx in symbol_to_index.items()}\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# Maximum sequence length\n",
    "max_seq_length = max_depth\n",
    "\n",
    "def encode_state(state):\n",
    "    # Convert symbols to indices\n",
    "    state_indices = [symbol_to_index[symbol] for symbol in state]\n",
    "    # Pad sequence\n",
    "    if len(state_indices) < max_seq_length:\n",
    "        state_indices += [symbol_to_index['PAD']] * (max_seq_length - len(state_indices))\n",
    "    else:\n",
    "        state_indices = state_indices[:max_seq_length]\n",
    "    return torch.tensor(state_indices, dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_symbols = list(library.keys())\n",
    "action_size = len(action_symbols)\n",
    "symbol_to_action_idx = {symbol: idx for idx, symbol in enumerate(action_symbols)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(action_symbols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 128\n",
    "hidden_dim = 256\n",
    "num_batches = 1000\n",
    "num_episodes_per_batch = 10\n",
    "batch_quantile = 0.1\n",
    "batch_size = 500\n",
    "gamma = 0.99\n",
    "epsilon_start = 1.0\n",
    "epsilon_end = 0.25\n",
    "epsilon_decay = 0.9995\n",
    "target_update = num_batches / 10\n",
    "memory_capacity = max_seq_length * num_episodes_per_batch * num_batches\n",
    "batch_eval = 10\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = DQNAgent(vocab_size, embedding_dim, hidden_dim, action_size)\n",
    "target_agent = DQNAgent(vocab_size, embedding_dim, hidden_dim, action_size)\n",
    "target_agent.load_state_dict(agent.state_dict())\n",
    "target_agent.eval()\n",
    "agent.train()\n",
    "\n",
    "optimizer = optim.Adam(agent.parameters(), lr=1e-4)\n",
    "criterion = nn.MSELoss()\n",
    "memory = ReplayBuffer(memory_capacity)\n",
    "\n",
    "epsilon = epsilon_start\n",
    "\n",
    "for batch in range(num_batches):\n",
    "    episodes = []\n",
    "    for episode in range(num_episodes_per_batch):\n",
    "        state_symbols = env.reset()\n",
    "        state_encoded = encode_state(state_symbols)  # Shape: (seq_length,)\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        transitions = []\n",
    "        i = 0\n",
    "\n",
    "        while not done and i < max_seq_length:\n",
    "            # Select action\n",
    "            action_idx = agent.act(state_encoded, epsilon)\n",
    "            action_symbol = action_symbols[action_idx]\n",
    "\n",
    "            try:\n",
    "                next_state_symbols, reward, done = env.step(action_symbol)\n",
    "            except ValueError as e:\n",
    "                reward = 0\n",
    "                done = True\n",
    "                next_state_symbols = state_symbols  # Remain in the same state\n",
    "\n",
    "            next_state_encoded = encode_state(next_state_symbols)\n",
    "            total_reward += reward\n",
    "\n",
    "            # Store transition\n",
    "            transitions.append((\n",
    "                state_encoded,\n",
    "                action_idx,\n",
    "                reward,\n",
    "                next_state_encoded,\n",
    "                done\n",
    "            ))\n",
    "\n",
    "            state_encoded = next_state_encoded\n",
    "            state_symbols = next_state_symbols\n",
    "\n",
    "            i += 1\n",
    "\n",
    "        if done:\n",
    "            print(f\"Batch {batch}.{episode} completed, Total Reward: {total_reward}\")\n",
    "        else:\n",
    "            total_reward = -1\n",
    "            print(f\"Batch {batch}.{episode} failed, Total Reward: {total_reward}\")\n",
    "\n",
    "        T = len(transitions)  # Total number of steps in the episode\n",
    "\n",
    "        # Compute G_t for each time step\n",
    "        transitions = [\n",
    "            (\n",
    "                t[0],  # state_encoded\n",
    "                t[1],  # action_idx\n",
    "                # total_reward * (gamma ** (T - idx - 1)),\n",
    "                total_reward,\n",
    "                t[3],  # next_state_encoded\n",
    "                t[4]   # done\n",
    "            )\n",
    "            for idx, t in enumerate(transitions)\n",
    "        ]\n",
    "\n",
    "        episodes.append((transitions, total_reward))\n",
    "\n",
    "    total_rewards = [episode[1] for episode in episodes]\n",
    "    threshold = np.quantile(total_rewards, 1 - batch_quantile)\n",
    "    top_episodes = [episode[0] for episode in episodes if episode[1] >= threshold]\n",
    "\n",
    "    # Store transitions from top episodes in the replay buffer\n",
    "    for episode_transitions in top_episodes:\n",
    "        for transition in episode_transitions:\n",
    "            memory.push(*transition)\n",
    "\n",
    "    # Experience replay\n",
    "    if len(memory) >= batch_size:\n",
    "        # Sample from memory\n",
    "        states_batch, actions_batch, rewards_batch, next_states_batch, dones_batch = memory.sample(batch_size)\n",
    "\n",
    "        try:\n",
    "            # Compute current Q-values\n",
    "            q_values = agent(states_batch)\n",
    "            q_values = q_values.gather(1, actions_batch.unsqueeze(1)).squeeze(1)\n",
    "\n",
    "            # Compute target Q-values\n",
    "            with torch.no_grad():\n",
    "                next_q_values = target_agent(next_states_batch).max(dim=1)[0]\n",
    "                target_q_values = rewards_batch + gamma * next_q_values * (1 - dones_batch)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = criterion(q_values, target_q_values)\n",
    "\n",
    "            # Optimize the model\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        except Exception as e:\n",
    "            print(f'Training failed due to {e}. Skipping this iteration...')\n",
    "\n",
    "    # Decay epsilon\n",
    "    if epsilon > epsilon_end:\n",
    "        epsilon *= epsilon_decay\n",
    "\n",
    "    # Update target network\n",
    "    if batch % target_update == 0:\n",
    "        target_agent.load_state_dict(agent.state_dict())\n",
    "\n",
    "    # Evaluation\n",
    "    if batch % batch_eval == 0:\n",
    "        print('---------------------')\n",
    "        print('Evaluating...')\n",
    "        print('---------------------')\n",
    "        agent.eval()\n",
    "        state_symbols = env.reset()\n",
    "        state_encoded = encode_state(state_symbols)\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        expression_actions = []\n",
    "        i = 0\n",
    "\n",
    "        while not done and i < max_seq_length:\n",
    "            with torch.no_grad():\n",
    "                q_values = agent(state_encoded.unsqueeze(0))  # Add batch dimension\n",
    "                action_idx = torch.argmax(q_values).item()\n",
    "            action_symbol = action_symbols[action_idx]\n",
    "            expression_actions.append(action_symbol)\n",
    "            try:\n",
    "                next_state_symbols, reward, done = env.step(action_symbol)\n",
    "            except ValueError as e:\n",
    "                reward = -1.0\n",
    "                done = True\n",
    "                next_state_symbols = state_symbols\n",
    "            next_state_encoded = encode_state(next_state_symbols)\n",
    "            total_reward += reward\n",
    "            state_encoded = next_state_encoded\n",
    "            state_symbols = next_state_symbols\n",
    "\n",
    "            i += 1\n",
    "\n",
    "        agent.train()  # Set back to training mode\n",
    "\n",
    "        if done and round(total_reward.item(), 2) == 1:\n",
    "            print('Found expression! Stopping early...')\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the agent\n",
    "state_symbols = env.reset()\n",
    "state_encoded = encode_state(state_symbols)\n",
    "done = False\n",
    "total_reward = 0\n",
    "expression_actions = []\n",
    "max_restart = 100\n",
    "r = 0\n",
    "i = 0\n",
    "\n",
    "agent.eval()\n",
    "target_agent.eval()\n",
    "\n",
    "while not done and r < max_restart:\n",
    "    with torch.no_grad():\n",
    "        q_values = agent(state_encoded.unsqueeze(0))  # Add batch dimension\n",
    "        action_idx = torch.argmax(q_values).item()\n",
    "    action_symbol = action_symbols[action_idx]\n",
    "    expression_actions.append(action_symbol)\n",
    "    \n",
    "    try:\n",
    "        next_state_symbols, reward, done = env.step(action_symbol)\n",
    "    except ValueError as e:\n",
    "        print(f'Error {e}. Exiting...')\n",
    "        reward = -1.0\n",
    "        done = True\n",
    "        next_state_symbols = state_symbols\n",
    "\n",
    "    next_state_encoded = encode_state(next_state_symbols)\n",
    "    total_reward += reward\n",
    "    state_encoded = next_state_encoded\n",
    "    state_symbols = next_state_symbols\n",
    "\n",
    "    if i == max_seq_length and not done:\n",
    "        state_symbols = env.reset()\n",
    "        state_encoded = encode_state(state_symbols)\n",
    "        total_reward = 0\n",
    "        expression_actions = []\n",
    "        i = 0\n",
    "        r += 1\n",
    "        print('restarting...')\n",
    "    else:\n",
    "        i += 1\n",
    "\n",
    "\n",
    "n_const = env.expression.n_constants\n",
    "const_count = 0\n",
    "for idx, token in enumerate(expression_actions):\n",
    "    if const_count == n_const:\n",
    "        break\n",
    "    if token == 'C':\n",
    "        const_val = env.expression.optimized_constants[const_count].item()\n",
    "        expression_actions[idx] = str(round(const_val, 3))\n",
    "        const_count += 1\n",
    "\n",
    "print(f\"Constructed Expression: {' '.join(expression_actions)}\")\n",
    "print(f\"Test Total Reward: {total_reward}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
